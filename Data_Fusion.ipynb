{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing for Fusion and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:12:53.854083600Z",
     "start_time": "2025-02-18T15:12:41.761926600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Model\n",
    "\n",
    "# Visualization\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T15:36:25.581539600Z",
     "start_time": "2025-02-18T15:36:25.297433200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clinical Datasets\n",
    "African = pd.read_csv(r\"C:\\Users\\dahab\\OneDrive\\Desktop\\T2D-Prediction-System--Data-Fusion-for-Enhanced-Decision-Making\\processed_datasets\\clinical\\African_pro.csv\")\n",
    "Bangladesh = pd.read_csv(r\"C:\\Users\\dahab\\OneDrive\\Desktop\\T2D-Prediction-System--Data-Fusion-for-Enhanced-Decision-Making\\processed_datasets\\clinical\\Bangladesh_pro.csv\")\n",
    "Iraq = pd.read_csv(r\"C:\\Users\\dahab\\OneDrive\\Desktop\\T2D-Prediction-System--Data-Fusion-for-Enhanced-Decision-Making\\processed_datasets\\clinical\\Iraq_pro.csv\")\n",
    "\n",
    "# Genetic Datasets\n",
    "inter_genetic = pd.read_csv(r\"C:\\Users\\dahab\\OneDrive\\Desktop\\T2D-Prediction-System--Data-Fusion-for-Enhanced-Decision-Making\\processed_datasets\\genetic\\inter_genetic_dataset.csv\")\n",
    "normal_genetic = pd.read_csv(r\"C:\\Users\\dahab\\OneDrive\\Desktop\\T2D-Prediction-System--Data-Fusion-for-Enhanced-Decision-Making\\processed_datasets\\genetic\\normal_genetic_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Patient number', 'Cholesterol', 'Glucose', 'HDL Chol',\n",
      "       'Chol/HDL ratio', 'Age', 'Gender', 'Height', 'Weight', 'BMI',\n",
      "       'Systolic BP', 'Diastolic BP', 'waist', 'hip', 'Waist/hip ratio',\n",
      "       'Diabetes', 'BMI Category'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['age', 'pulse_rate', 'systolic_bp', 'diastolic_bp', 'glucose', 'height',\n",
      "       'weight', 'bmi', 'family_diabetes', 'hypertensive',\n",
      "       'family_hypertension', 'cardiovascular_disease', 'stroke',\n",
      "       'gender_Encoded', 'diabetic_Encoded'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['Age', 'Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI',\n",
      "       'Gender_Encoded', 'Class_Encoded'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['STUDY', 'DISEASE_DESCRIPTION', 'REGION', 'CHR_ID', 'CHR_POS',\n",
      "       'MAPPED_GENE', 'UPSTREAM_GENE_ID', 'DOWNSTREAM_GENE_ID',\n",
      "       'UPSTREAM_GENE_DISTANCE', 'DOWNSTREAM_GENE_DISTANCE', 'SNPS', 'MERGED',\n",
      "       'GENOMIC_CONTEXT', 'INTERGENIC', 'RISK_ALLELE_FREQUENCY', 'PVALUE',\n",
      "       'PVALUE_MLOG', 'EFFECT_SIZE', 'CASE_PERCENTAGE', 'CI_LOWER_BOUND',\n",
      "       'CI_UPPER_BOUND', 'CI_RANGE', 'SNPS_PASSING_QC', 'PLATFORM_AFFYMETRIX',\n",
      "       'PLATFORM_AFFYMETRIX_ILLUMINA', 'PLATFORM_ILLUMINA', 'IMPUTED_ENCODED',\n",
      "       'RISK_ALLELE_ENCODED'],\n",
      "      dtype='object') \n",
      "\n",
      "Index(['STUDY', 'DISEASE_DESCRIPTION', 'REGION', 'CHR_ID', 'CHR_POS',\n",
      "       'MAPPED_GENE', 'SNP_GENE_IDS', 'SNPS', 'MERGED', 'GENOMIC_CONTEXT',\n",
      "       'INTERGENIC', 'RISK_ALLELE_FREQUENCY', 'PVALUE', 'PVALUE_MLOG',\n",
      "       'EFFECT_SIZE', 'CASE_PERCENTAGE', 'CI_LOWER_BOUND', 'CI_UPPER_BOUND',\n",
      "       'CI_RANGE', 'SNPS_PASSING_QC', 'PLATFORM_AFFYMETRIX',\n",
      "       'PLATFORM_AFFYMETRIX_ILLUMINA', 'PLATFORM_ILLUMINA', 'IMPUTED_ENCODED',\n",
      "       'RISK_ALLELE_ENCODED'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(African.columns, \"\\n\")\n",
    "print(Bangladesh.columns, \"\\n\")\n",
    "print(Iraq.columns, \"\\n\")\n",
    "print(inter_genetic.columns, \"\\n\")\n",
    "print(normal_genetic.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniting Target Columns' name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "African.rename(columns={\n",
    "    'Diabetes': 'T2D',\n",
    "}, inplace=True)\n",
    " \n",
    "Bangladesh.rename(columns={\n",
    "    'diabetic_Encoded': 'T2D',\n",
    "}, inplace=True)\n",
    " \n",
    "Iraq.rename(columns={\n",
    "    'Class_Encoded': 'T2D',\n",
    "}, inplace=True)\n",
    " \n",
    "# Add a new column 'T2D' with all values set to 1\n",
    "#inter_genetic['T2D'] = 1\n",
    " \n",
    "# Add a new column 'T2D' with all values set to 1\n",
    "#normal_genetic['T2D'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient number       int64\n",
      "Cholesterol          int64\n",
      "Glucose              int64\n",
      "HDL Chol             int64\n",
      "Chol/HDL ratio     float64\n",
      "Age                  int64\n",
      "Gender               int64\n",
      "Height               int64\n",
      "Weight               int64\n",
      "BMI                float64\n",
      "Systolic BP          int64\n",
      "Diastolic BP         int64\n",
      "waist                int64\n",
      "hip                  int64\n",
      "Waist/hip ratio    float64\n",
      "T2D                  int64\n",
      "BMI Category         int64\n",
      "dtype: object \n",
      "\n",
      "age                         int64\n",
      "pulse_rate                  int64\n",
      "systolic_bp                 int64\n",
      "diastolic_bp                int64\n",
      "glucose                   float64\n",
      "height                    float64\n",
      "weight                    float64\n",
      "bmi                       float64\n",
      "family_diabetes             int64\n",
      "hypertensive                int64\n",
      "family_hypertension         int64\n",
      "cardiovascular_disease      int64\n",
      "stroke                      int64\n",
      "gender_Encoded              int64\n",
      "T2D                         int64\n",
      "dtype: object \n",
      "\n",
      "Age                 int64\n",
      "Urea              float64\n",
      "Cr                  int64\n",
      "HbA1c             float64\n",
      "Chol              float64\n",
      "TG                float64\n",
      "HDL               float64\n",
      "LDL               float64\n",
      "VLDL              float64\n",
      "BMI               float64\n",
      "Gender_Encoded      int64\n",
      "T2D                 int64\n",
      "dtype: object \n",
      "\n",
      "STUDY                            object\n",
      "DISEASE_DESCRIPTION              object\n",
      "REGION                           object\n",
      "CHR_ID                            int64\n",
      "CHR_POS                           int64\n",
      "MAPPED_GENE                      object\n",
      "UPSTREAM_GENE_ID                  int64\n",
      "DOWNSTREAM_GENE_ID                int64\n",
      "UPSTREAM_GENE_DISTANCE            int64\n",
      "DOWNSTREAM_GENE_DISTANCE          int64\n",
      "SNPS                             object\n",
      "MERGED                            int64\n",
      "GENOMIC_CONTEXT                  object\n",
      "INTERGENIC                        int64\n",
      "RISK_ALLELE_FREQUENCY           float64\n",
      "PVALUE                          float64\n",
      "PVALUE_MLOG                     float64\n",
      "EFFECT_SIZE                     float64\n",
      "CASE_PERCENTAGE                 float64\n",
      "CI_LOWER_BOUND                  float64\n",
      "CI_UPPER_BOUND                  float64\n",
      "CI_RANGE                        float64\n",
      "SNPS_PASSING_QC                   int64\n",
      "PLATFORM_AFFYMETRIX               int64\n",
      "PLATFORM_AFFYMETRIX_ILLUMINA      int64\n",
      "PLATFORM_ILLUMINA                 int64\n",
      "IMPUTED_ENCODED                   int64\n",
      "RISK_ALLELE_ENCODED               int64\n",
      "dtype: object \n",
      "\n",
      "STUDY                            object\n",
      "DISEASE_DESCRIPTION              object\n",
      "REGION                           object\n",
      "CHR_ID                            int64\n",
      "CHR_POS                           int64\n",
      "MAPPED_GENE                      object\n",
      "SNP_GENE_IDS                      int64\n",
      "SNPS                             object\n",
      "MERGED                            int64\n",
      "GENOMIC_CONTEXT                  object\n",
      "INTERGENIC                        int64\n",
      "RISK_ALLELE_FREQUENCY           float64\n",
      "PVALUE                          float64\n",
      "PVALUE_MLOG                     float64\n",
      "EFFECT_SIZE                     float64\n",
      "CASE_PERCENTAGE                 float64\n",
      "CI_LOWER_BOUND                  float64\n",
      "CI_UPPER_BOUND                  float64\n",
      "CI_RANGE                        float64\n",
      "SNPS_PASSING_QC                   int64\n",
      "PLATFORM_AFFYMETRIX               int64\n",
      "PLATFORM_AFFYMETRIX_ILLUMINA      int64\n",
      "PLATFORM_ILLUMINA                 int64\n",
      "IMPUTED_ENCODED                   int64\n",
      "RISK_ALLELE_ENCODED               int64\n",
      "dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(African.dtypes, '\\n')\n",
    "print(Bangladesh.dtypes, '\\n')\n",
    "print(Iraq.dtypes, '\\n')\n",
    "print(inter_genetic.dtypes, '\\n')\n",
    "print(normal_genetic.dtypes, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Generator model\n",
    "def build_generator(input_dim, output_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        BatchNormalization(),\n",
    "        Dense(output_dim, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define Discriminator model\n",
    "def build_discriminator(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=input_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid')  # Output probability (Real vs Fake)\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Compile the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_inter_genetic_scaled.shape[1]  # Number of features\n",
    "generator = build_generator(100, input_dim)  # Generator takes 100 noise dimensions\n",
    "discriminator = build_discriminator(input_dim)\n",
    "\n",
    "# Create GAN model (combining Generator & Discriminator)\n",
    "discriminator.trainable = False  # Freeze discriminator during GAN training\n",
    "\n",
    "gan = Sequential([generator, discriminator])\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Epoch 0: D Loss: 0.9932, G Loss: 0.9932\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 1: D Loss: 0.9932, G Loss: 0.9932\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Epoch 2: D Loss: 0.9932, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Epoch 3: D Loss: 0.9932, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Epoch 4: D Loss: 0.9933, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Epoch 5: D Loss: 0.9933, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 6: D Loss: 0.9933, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Epoch 7: D Loss: 0.9933, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Epoch 8: D Loss: 0.9933, G Loss: 0.9933\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n",
      "Epoch 9: D Loss: 0.9933, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Epoch 10: D Loss: 0.9933, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Epoch 11: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Epoch 12: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Epoch 13: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 14: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Epoch 15: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Epoch 16: D Loss: 0.9934, G Loss: 0.9934\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Epoch 17: D Loss: 0.9934, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Epoch 18: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Epoch 19: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Epoch 20: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
      "Epoch 21: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Epoch 22: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Epoch 23: D Loss: 0.9935, G Loss: 0.9935\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Epoch 24: D Loss: 0.9935, G Loss: 0.9936\n"
     ]
    }
   ],
   "source": [
    "def train_gan(epochs, batch_size):\n",
    "    real_labels = np.ones((batch_size, 1))  # Label 1 for real data (Yes)\n",
    "    fake_labels = np.zeros((batch_size, 1))  # Label 0 for fake data (No)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Select a random batch of real samples\n",
    "        idx = np.random.randint(0, X_train_inter_genetic_scaled.shape[0], batch_size)\n",
    "        real_samples = X_train_inter_genetic_scaled[idx]\n",
    "\n",
    "        # Generate fake samples\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))  # Noise input\n",
    "        fake_samples = generator.predict(noise)\n",
    "\n",
    "        # Train the Discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)\n",
    "\n",
    "        # Ensure we extract the actual loss value (first element of returned list/tuple)\n",
    "        d_loss_real_value = d_loss_real[0] if isinstance(d_loss_real, (list, tuple, np.ndarray)) else d_loss_real\n",
    "        d_loss_fake_value = d_loss_fake[0] if isinstance(d_loss_fake, (list, tuple, np.ndarray)) else d_loss_fake\n",
    "        d_loss = 0.5 * (d_loss_real_value + d_loss_fake_value)  # Compute average loss\n",
    "\n",
    "        # Train the Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        g_loss_data = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "        # Extract loss value from g_loss_data if it's a list/tuple\n",
    "        g_loss = g_loss_data[0] if isinstance(g_loss_data, (list, tuple, np.ndarray)) else g_loss_data\n",
    "\n",
    "        # Print loss every 500 epochs  \n",
    "        print(f\"Epoch {epoch}: D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=25, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Synthetic \"No\" Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Generated dataset saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Ensure Generator Inputs Match Expected Noise Dim\n",
    "num_samples = 1110\n",
    "noise_dim = generator.input_shape[1]  # Get correct input size\n",
    "noise = np.random.normal(0, 1, (num_samples, noise_dim))  # Generate noise\n",
    "\n",
    "# Step 2: Generate Fake \"No\" Samples\n",
    "synthetic_no_samples = generator.predict(noise)\n",
    "\n",
    "# Step 3: Apply inverse_transform only on numerical columns\n",
    "synthetic_no_samples_scaled = scaler.inverse_transform(synthetic_no_samples[:, :scaler.scale_.shape[0]])\n",
    "\n",
    "# Step 4: Convert to DataFrame with Correct Column Names\n",
    "synthetic_df = pd.DataFrame(synthetic_no_samples_scaled, columns=inter_genetic.columns[:scaler.scale_.shape[0]])\n",
    "\n",
    "# Step 5: Add Missing Categorical Columns Randomly\n",
    "for col in inter_genetic.columns[scaler.scale_.shape[0]:-1]:  # Exclude target column\n",
    "    synthetic_df[col] = np.random.choice(inter_genetic[col].values, size=len(synthetic_df))\n",
    "\n",
    "# Step 6: Label Synthetic Samples as \"No\" (0)\n",
    "synthetic_df['Diabetes'] = 0\n",
    "\n",
    "# Step 7: Label Original Samples as \"Yes\" (1)\n",
    "inter_genetic['Diabetes'] = 1\n",
    "\n",
    "# Step 8: Combine Real and Synthetic Data\n",
    "balanced_df = pd.concat([inter_genetic, synthetic_df], ignore_index=True)\n",
    "\n",
    "# Step 9: Save to CSV\n",
    "balanced_df.to_csv(\"diabetes_balanced_data_gan.csv\", index=False)\n",
    "print(\"Generated dataset saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diabetes\n",
       "1    1110\n",
       "0    1000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['Diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting datasets into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting African dataset\n",
    "X_train_african, X_test_african, y_train_african, y_test_african = train_test_split(\n",
    "    African.drop('T2D', axis=1), African['T2D'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Splitting Bangladesh dataset\n",
    "X_train_bangladesh, X_test_bangladesh, y_train_bangladesh, y_test_bangladesh = train_test_split(\n",
    "    Bangladesh.drop('T2D', axis=1), Bangladesh['T2D'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Splitting Iraq dataset\n",
    "X_train_iraq, X_test_iraq, y_train_iraq, y_test_iraq = train_test_split(\n",
    "    Iraq.drop('T2D', axis=1), Iraq['T2D'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Splitting Inter Genetic dataset\n",
    "X_train_inter_genetic, X_test_inter_genetic, y_train_inter_genetic, y_test_inter_genetic = train_test_split(\n",
    "    inter_genetic.drop('T2D', axis=1), inter_genetic['T2D'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Splitting Normal Genetic dataset\n",
    "X_train_normal_genetic, X_test_normal_genetic, y_train_normal_genetic, y_test_normal_genetic = train_test_split(\n",
    "    normal_genetic.drop('T2D', axis=1), normal_genetic['T2D'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Initializing the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Handling non-numeric data by selecting numeric columns only\n",
    "def preprocess_and_scale(X_train, X_test):\n",
    "    # Selecting only numeric columns\n",
    "    X_train_numeric = X_train.select_dtypes(include=[float, int])\n",
    "    X_test_numeric = X_test.select_dtypes(include=[float, int])\n",
    "    \n",
    "    # Scaling numeric features\n",
    "    X_train_scaled = scaler.fit_transform(X_train_numeric)\n",
    "    X_test_scaled = scaler.transform(X_test_numeric)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Preprocessing and scaling for African dataset\n",
    "X_train_african_scaled, X_test_african_scaled = preprocess_and_scale(\n",
    "    X_train_african, X_test_african\n",
    ")\n",
    "\n",
    "# Preprocessing and scaling for Bangladesh dataset\n",
    "X_train_bangladesh_scaled, X_test_bangladesh_scaled = preprocess_and_scale(\n",
    "    X_train_bangladesh, X_test_bangladesh\n",
    ")\n",
    "\n",
    "# Preprocessing and scaling for Iraq dataset\n",
    "X_train_iraq_scaled, X_test_iraq_scaled = preprocess_and_scale(\n",
    "    X_train_iraq, X_test_iraq\n",
    ")\n",
    "\n",
    "# Preprocessing and scaling for Inter Genetic dataset\n",
    "X_train_inter_genetic_scaled, X_test_inter_genetic_scaled = preprocess_and_scale(\n",
    "    X_train_inter_genetic, X_test_inter_genetic\n",
    ")\n",
    "\n",
    "# Preprocessing and scaling for Normal Genetic dataset\n",
    "X_train_normal_genetic_scaled, X_test_normal_genetic_scaled = preprocess_and_scale(\n",
    "    X_train_normal_genetic, X_test_normal_genetic\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Defining datasets and corresponding scaled data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for model selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Defining datasets and corresponding scaled data\n",
    "datasets = {\n",
    "    \"African\": (X_train_african_scaled, X_test_african_scaled, y_train_african, y_test_african),\n",
    "    \"Bangladesh\": (X_train_bangladesh_scaled, X_test_bangladesh_scaled, y_train_bangladesh, y_test_bangladesh),\n",
    "    \"Iraq\": (X_train_iraq_scaled, X_test_iraq_scaled, y_train_iraq, y_test_iraq),\n",
    "    \"Inter Genetic\": (X_train_inter_genetic_scaled, X_test_inter_genetic_scaled, y_train_inter_genetic, y_test_inter_genetic),\n",
    "    \"Normal Genetic\": (X_train_normal_genetic_scaled, X_test_normal_genetic_scaled, y_train_normal_genetic, y_test_normal_genetic)\n",
    "}\n",
    "\n",
    "# Models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss'),\n",
    "}\n",
    "\n",
    "# Looping through each dataset and evaluating models\n",
    "for dataset_name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"\\n{dataset_name} Dataset Model Selection Results:\")\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)  # Training the model\n",
    "        y_pred = model.predict(X_test)  # Predicting on the test set\n",
    "        accuracy = accuracy_score(y_test, y_pred)  # Calculating accuracy\n",
    "        results[model_name] = accuracy\n",
    "\n",
    "    # Displaying results for the current dataset\n",
    "    for model_name, accuracy in results.items():\n",
    "        print(f\"{model_name}: Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
